# -*- coding: utf-8 -*-
"""Nessma news scrapped/french .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IDJZ5FhowEyMhP4_b1bIyBCfxVoQQ-56

**This notebook is a simple uncommented draft**
"""

import bs4 as bs
from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as soup
site = 'https://www.nessma.tv/fr'
uClient= uReq(site)
page_html=uClient.read()
uClient.close()
page_soup = soup(page_html,"html.parser")
containers= page_soup.findAll("div",{"class":"taxo-menu"},{"id":"menu-principale"})
categories={}
for url in containers:
     ch=url.a.text
     ch=ch.replace('\n\t\t\t\t\t\t\t\t\t\t','')
     if url.a['href']!= 'javascript:void(0)' :
        categories[ch]= url.a["href"]

print(categories)
titles=[]
for key in categories.keys():
  titles.append(key)

print(titles)

"""**prototyping**

extracting articles and titles from different pages of the same category
"""

i=1
titles2=[]
articles=[]
M=[[string , string]]
while i<5 :
    site = 'https://www.nessma.tv/ar/%D8%A3%D8%AE%D8%A8%D8%A7%D8%B1-%D8%A7%D9%84%D8%AC%D9%87%D8%A7%D8%AA/26?page='+str(i)
    uClient= uReq(site)
    page_html=uClient.read()
    uClient.close()
    page_soup2 = soup(page_html,"html.parser")
    containers2=page_soup2.findAll('h2',{'class':'entry-title'})
    for T in containers2 :
      articles.append(T.a["href"])
      titles2.append(T.text)
      M[i][0]=T.text
      M[i][1]=T.a["href"]
    i+=1
    print(site)
print(articles) 
print(titles2) 
print(len(titles2))
print(len(articles))

"""extract: puts all articles and titles respictively in a"""

import numpy as np
import string

def extract (P) :
  
  titles2=[]
  articles=[]
  M=[[string , string]]
  for i in range (10):
     s=i
     site = P+str(i)
     uClient= uReq(site)
     page_html=uClient.read()
     uClient.close()
     page_soup2 = soup(page_html,"html.parser")
     containers2=page_soup2.findAll('h2',{'class':'entry-title'})
     for T in containers2 :
       articles.append(T.a["href"])
       ch=T.text
       ch=ch.replace('\n','')
       titles2.append(ch)
       M.append([ch,T.a["href"]])
       
  return(np.mat(M))    
L = extract('https://www.nessma.tv/fr/%D8%A3%D8%AE%D8%A8%D8%A7%D8%B1-%D8%A7%D9%84%D8%AC%D9%87%D8%A7%D8%AA/26?page=')
print(L)
print(len(L))

"""going through all pages of all categoris , extracting articles and titles
as I have no idea what kind of data format we need,I'm opting for all kinds and will delete the unecessary ones later.
"""

cat={}
for ca in categories.keys() :
  cat[ca]=[categories[ca] , extract(categories[ca]+'?page=') ]
print(cat)